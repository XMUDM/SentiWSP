import os, sys, random
from pathlib import Path
from datetime import datetime, timezone, timedelta
import numpy as np
import torch
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch import nn
import torch.nn.functional as F
import datasets
from transformers import ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining, get_linear_schedule_with_warmup, ElectraModel
from _utils.utils import *
from _utils.would_like_to_pr import *
from tqdm import tqdm
import faiss
import json
import argparse
from transformers import WEIGHTS_NAME, CONFIG_NAME
import pickle
# parser config
parser = argparse.ArgumentParser(description='ANN data generate configuration')
parser.add_argument('--model', default='electra',help='pre train model type')
parser.add_argument('--model_path', default='rawbase',help='model path required to generate Ann data')
parser.add_argument('--warm_name', default='largesen',help='model path required to generate Ann data')
parser.add_argument('--size', type=str,default='large',help='the size of model')
parser.add_argument('--tokenizer_path', default='./pretrain_model/electra/large/',help='tokenizer path required to generate Ann data')
parser.add_argument('--max_length',type=int, default = 128,help='the max_length of input sequence')
parser.add_argument('--Neg_File_name', default='sen',help='Last generated Ann hard neg file path')
parser.add_argument('--negative_num', type=int,default=7,help='Last generated Ann hard neg file path')
parser.add_argument('--ANN_File_path', default='./ANCEdata/',help='The path of the query positve generated by sampling to the data file')
parser.add_argument('--gpu_num', type=int ,default = 4 ,help='Number of GPUs used in training')
# parser.add_argument('--dpp_gpu_num', default="0, 1",help='available GPU numbers in a multi GPU environment')
parser.add_argument('--single_gpunum', default='0',help='GPU number used in single GPU environment')
parser.add_argument('--save_model', type=bool ,default = True ,help='wheather you want to save the model')
parser.add_argument('--save_model_path', default='./save_ance/',help='the save dir of fine-tunning model path')
parser.add_argument('--save_model_name', default='ANCE_checkpoint',help='model name')
parser.add_argument('--sentimask_prob', type=float ,default = 0.7 ,help='Proportion of emotional words mask')

parser.add_argument('--batch_size',type=int ,default=32,help='the batch size of training process')
parser.add_argument('--max_epoch', type=int,default=10,help='the max epoch of training process')
parser.add_argument('--lr', default = 1e-5,help='the learning rate of training process')
parser.add_argument('--eps', default = 1e-8,help='the eps of training process')
parser.add_argument('--pretrain_model', type=str,default='./pretrain_model/',help='pre train model path')
parser.add_argument('--simcal', type=str,default='cos',help='The Similarity of loss calculate[dot/cos]')
parser.add_argument('--losscal', type=str,default='nll',help='The loss calculate[nll/triplet]')

parser.add_argument("--rank", type=int,default=-1, help="rank")
parser.add_argument("--local_rank", type=int,default=-1, help="local rank")
args = parser.parse_args()

# seed
random_seed = 2022
random.seed(random_seed)
torch.manual_seed(random_seed)

ann_data=args.ANN_File_path+args.warm_name+'_'+args.size+str(args.sentimask_prob)+'/wiki'+str(args.sentimask_prob)+'_ANN.json'
neg_ann_path=args.ANN_File_path+args.warm_name+'_'+args.size+str(args.sentimask_prob)+'/wiki_ANN_'+args.Neg_File_name+str(args.negative_num)+'_neg.npy'
model_path=args.pretrain_model+args.model_path
# mult GPU
if  args.gpu_num > 1:
    #os.environ["CUDA_VISIBLE_DEVICES"] = args.dpp_gpu_num
    torch.distributed.init_process_group(backend="nccl")

    local_rank = torch.distributed.get_rank()
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)
else:
    device = torch.device("cuda:" + str(args.single_gpunum) if torch.cuda.is_available() else "cpu")
    
# Load model settings
disc_config = ElectraConfig.from_pretrained(model_path)

# Define Ann triplet data type
class ANNTripletTrainingData(torch.utils.data.Dataset):

    def __init__(self, ANN_Neg_ID_File, ANN_File):
        
        # Load negative sample sequence number
        self.query_negative_passage = np.load(ANN_Neg_ID_File,allow_pickle=True).item()
        
        # Ann data after pad loading (including query and positive, and positive is the original data)
        self.ann_data = []
        for line in open(ANN_File, 'rb'):
            example_dict = {}
            row = json.loads(line)
            for key,value in row.items():
                example_dict[key] = value
            self.ann_data.append(example_dict)
        
        self.ANN_Triplet_data = []
        
        for index,data in enumerate(self.ann_data):
            example_dict = {}
            example_dict['positive'] = data['positive']
            example_dict['query'] = data['query']
            example_dict['sentence_len'] = data['sentence_len']
            for neg_id in self.query_negative_passage[index]:
                example_dict['negative'] = self.ann_data[neg_id]['positive']
                example_dict['neg_sentence_len'] = self.ann_data[neg_id]['sentence_len']
                self.ANN_Triplet_data.append(example_dict)
        
        self.size = len(self.ANN_Triplet_data)

    def __getitem__(self, index):
        item = self.ANN_Triplet_data[index]
        
        pos_attention_mask = [1] * item['sentence_len'] + [0] * (args.max_length - item['sentence_len'])
        q_attention_mask = pos_attention_mask
        neg_attention_mask = [1] * item['neg_sentence_len'] + [0] * (args.max_length - item['neg_sentence_len'])
        q_input_id = torch.tensor(item['query'], dtype=torch.int)
        pos_input_id = torch.tensor(item['positive'], dtype=torch.int)
        neg_input_id = torch.tensor(item['negative'], dtype=torch.int)
        q_attention_mask = torch.tensor(q_attention_mask, dtype=torch.bool)
        pos_attention_mask = torch.tensor(pos_attention_mask, dtype=torch.bool)
        neg_attention_mask = torch.tensor(neg_attention_mask, dtype=torch.bool)
        return q_input_id, q_attention_mask, pos_input_id, pos_attention_mask, neg_input_id, neg_attention_mask

    def __len__(self):
        return self.size

class DotProductSimilarity(nn.Module):
    def __init__(self,scale_output=False):
        super(DotProductSimilarity,self).__init__()
        self.scale_output=scale_output
    def forward(self,tensor_1,tensor_2):
        result=(tensor_1*tensor_2).sum(dim=-1)
        if(self.scale_output):
            result/=math.sqrt(tensor_1.size(-1))
        return  result

'''
ANN model definition
'''
class ELEDIC_NLL_LN(nn.Module):

    def __init__(self, pretrained_model,temp=0.05,margin=0.2):
        super(ELEDIC_NLL_LN, self).__init__()
        self.DModel = pretrained_model
        self.norm = nn.LayerNorm(disc_config.hidden_size)
        self.temp=temp
        self.margin=margin
    def get_emb(self, input_ids, attention_mask, pooling='cls'):
        out = self.DModel(input_ids=input_ids,
                                attention_mask=attention_mask)
        # Decide how to get the vector after the query passes through the model
        if pooling == 'cls':
            return self.norm(out.last_hidden_state[:, 0])  # [batch_size, hidden_size]
        
        if pooling == 'last-avg':
            last = out.last_hidden_state.transpose(1, 2)    # [batch_size, hidden_size, seq_len]
            return self.norm(torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1))      # [batch_size, hidden_size]
        
        if pooling == 'first-last-avg':
            first = out.hidden_states[1].transpose(1, 2)    # [batch_size, hidden_size, seq_len]
            last = out.hidden_states[-1].transpose(1, 2)    # [batch_size, hidden_size, seq_len]                  
            first_avg = torch.avg_pool1d(first, kernel_size=last.shape[-1]).squeeze(-1) # [batch_size, hidden_size]
            last_avg = torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)   # [batch_size, hidden_size]
            avg = torch.cat((first_avg.unsqueeze(1), last_avg.unsqueeze(1)), dim=1)     # [batch_size,2, hidden_size]
            return self.norm(torch.avg_pool1d(avg.transpose(1, 2), kernel_size=2).squeeze(-1))   # [batch_size, hidden_size]
    
    def forward(self,query_ids,attention_mask_q,
                input_ids_pos,attention_mask_pos,
                input_ids_neg,attention_mask_neg):
        
        '''
        query_ids query
        input_ids_pos positive_passage
        input_ids_neg negative_passage
        '''
        q_embs = self.get_emb(query_ids, attention_mask_q)
        pos_embs = self.get_emb(input_ids_pos, attention_mask_pos)
        neg_embs = self.get_emb(input_ids_neg, attention_mask_neg)

        if args.losscal=='nll':
            batch_size=q_embs.size(0)
            y_pred = torch.stack([q_embs, pos_embs, neg_embs], dim=1)
            y_pred = y_pred.reshape(batch_size * 3, -1)
            y_true = torch.arange(y_pred.shape[0], device=device)
            use_row = torch.where((y_true + 1) % 3 != 0)[0]
            y_true = (use_row - use_row % 3 * 2) + 1

            if args.simcal=='cos':
                sim = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=-1)
            elif args.simcal=='dot':
                dot = DotProductSimilarity()
                sim = dot(y_pred.unsqueeze(1),y_pred.unsqueeze(0))  
            else:
                sys.exit()  
            
            sim = sim - torch.eye(y_pred.shape[0], device=device) * 1e12
            sim = torch.index_select(sim, 0, use_row)
            sim = sim / self.temp
            loss = F.cross_entropy(sim, y_true)

        elif args.losscal=='triplet':
            
            if args.simcal=='cos':
                pos_dist = F.cosine_similarity(q_embs,pos_embs) 
                neg_dist = F.cosine_similarity(q_embs,neg_embs)
            elif args.simcal=='dot':
                dot = DotProductSimilarity()
                pos_dist = dot(q_embs,pos_embs) 
                neg_dist = dot(q_embs,neg_embs)
            else: 
                sys.exit()
            loss = pos_dist - neg_dist + self.margin
            loss = loss.mean()
        else:
            sys.exit()
        return (loss,)
    
    def save_DModel(self,output_dir):
        if os.path.exists(output_dir) == False:
            os.makedirs(output_dir)
        model_to_save = self.DModel
        output_model_file = os.path.join(output_dir, WEIGHTS_NAME)
        output_config_file = os.path.join(output_dir, CONFIG_NAME)
        torch.save(model_to_save.state_dict(), output_model_file)
        model_to_save.config.to_json_file(output_config_file)
        print("save model in :"+output_dir)

        
def train(model, train_loader):
    print("***** Running training *****")
    model.train()
    model.zero_grad()
    
    optimizer = torch.optim.AdamW(params=model.parameters(), lr=args.lr, eps=args.eps)
    total_steps = len(train_loader) * args.max_epoch
    warmup_steps = 0.1 * len(train_loader) / args.gpu_num
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps = warmup_steps, # Default value in run_glue.py
                                                num_training_steps = total_steps)
    step=0
    loss_list=[]
    for epoch in range(args.max_epoch):
        total_train_loss = 0
        for iter_num, batch in enumerate(tqdm(train_loader)):
            step += 1
            batch = tuple(t.to(device) for t in batch)
            inputs = {
                    "query_ids": batch[0].long(),
                    "attention_mask_q": batch[1].long(),
                    "input_ids_pos": batch[2].long(),
                    "attention_mask_pos": batch[3].long(),
                    "input_ids_neg": batch[4].long(),
                    "attention_mask_neg": batch[5].long()}
            outputs = model(**inputs)
            loss = outputs[0]
            total_train_loss += loss.item()
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)
            optimizer.step()
            scheduler.step()  # Update learning rate schedule
            model.zero_grad()
            if step % 20 == 0:
                if dist.get_rank() == 0:
                    loss_list.append(loss.item())
                print(">>> epoth: %d, iter_num: %d, loss: %.4f" % (epoch, step, loss.item()))
            if step == 200:
                if dist.get_rank() == 0:
                    if args.save_model:
                        output_dir = args.save_model_path
                        save_model(model, output_dir, step)
            if step % 1000 == 0 and step > 0:
                if dist.get_rank() == 0:
                    if args.save_model:
                        output_dir = args.save_model_path
                        save_model(model, output_dir, step)
                if step == 2000:
                    if dist.get_rank() == 0:
                        with open('./loss_ance.pkl', 'wb') as f:
                            pickle.dump(loss_list, f, pickle.HIGHEST_PROTOCOL)
                    sys.exit()

        if dist.get_rank() == 0:
            if args.save_model:
                output_dir = args.save_model_path
                save_model(model, output_dir, epoch + 1)

        print("Epoch: %d, Average training loss: %.4f" %(epoch, total_train_loss/len(train_loader)))
    
def save_model(model,output_dir,flag):
    if flag>100:
        save_dis_path=output_dir + str(flag) + 'iter_'+args.save_model_name
    else:
        save_dis_path = output_dir + str(flag) + 'epoch_'+args.save_model_name
    if os.path.exists(save_dis_path) == False:
        os.makedirs(save_dis_path)
    if args.gpu_num > 1:
        model.module.save_DModel(save_dis_path)
    else:
        model.save_DModel(save_dis_path)

if __name__ == "__main__":
    ann_dataset = ANNTripletTrainingData(neg_ann_path, ann_data)
    ann_dataloader = torch.utils.data.DataLoader(ann_dataset, batch_size=args.batch_size)
    
    if args.gpu_num > 1:
        sampler = DistributedSampler(ann_dataset)
        ann_dataloader = torch.utils.data.DataLoader(ann_dataset, batch_size=args.batch_size,sampler=sampler)
    else:
        ann_dataloader = torch.utils.data.DataLoader(ann_dataset, batch_size=args.batch_size)

    print("starting using ann data to train...")
    print("start train model in "+ model_path)
    discmodel = ElectraModel.from_pretrained(model_path)
    annmodel = ELEDIC_NLL_LN(discmodel)
    annmodel.to(device)
    if  args.gpu_num > 1:
        if torch.cuda.device_count() > 1:
            print("Let's use", torch.cuda.device_count(), "GPUs!")
            annmodel = torch.nn.parallel.DistributedDataParallel(annmodel,
                                                              device_ids=[local_rank],
                                                              output_device=local_rank)
        else:
            print("There are not enough GPUs available!")
            sys.exit()
    train(annmodel, ann_dataloader)